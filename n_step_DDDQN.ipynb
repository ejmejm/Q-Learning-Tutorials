{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "# env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "print('Action space:', env.action_space)\n",
    "print('Observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FRAMES = 4\n",
    "\n",
    "def filter_obs(obs, resize_shape=(84, 110), crop_shape=None):\n",
    "    assert(type(obs) == np.ndarray), \"The observation must be a numpy array!\"\n",
    "    assert(len(obs.shape) == 3), \"The observation must be a 3D array!\"\n",
    "\n",
    "    obs = cv2.resize(obs, resize_shape, interpolation=cv2.INTER_LINEAR)\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "    obs = obs / 255.\n",
    "    if crop_shape:\n",
    "        crop_x_margin = (resize_shape[1] - crop_shape[1]) // 2\n",
    "        crop_y_margin = (resize_shape[0] - crop_shape[0]) // 2\n",
    "        \n",
    "        x_start, x_end = crop_x_margin, resize_shape[1] - crop_x_margin\n",
    "        y_start, y_end = crop_y_margin, resize_shape[0] - crop_y_margin\n",
    "        \n",
    "        obs = obs[x_start:x_end, y_start:y_end]\n",
    "    \n",
    "    return obs\n",
    "\n",
    "def get_stacked_obs(obs, prev_frames):\n",
    "    if not prev_frames:\n",
    "        prev_frames = [obs] * (N_FRAMES - 1)\n",
    "        \n",
    "    prev_frames.append(obs)\n",
    "    stacked_frames = np.stack(prev_frames)\n",
    "    prev_frames = prev_frames[-(N_FRAMES-1):]\n",
    "    \n",
    "    return stacked_frames, prev_frames\n",
    "\n",
    "def preprocess_obs(obs, prev_frames):\n",
    "    filtered_obs = filter_obs(obs)\n",
    "    stacked_obs, prev_frames = get_stacked_obs(filtered_obs, prev_frames)\n",
    "    return stacked_obs, prev_frames\n",
    "\n",
    "def format_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Paper\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_acts):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(N_FRAMES, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.value_layer1 = nn.Sequential(\n",
    "            nn.Linear(64 * 10 * 7, 512),\n",
    "            nn.ReLU())\n",
    "        self.value_layer2 = nn.Linear(512, 1)\n",
    "\n",
    "        self.advantages_layer1 = nn.Sequential(\n",
    "            nn.Linear(64 * 10 * 7, 512),\n",
    "            nn.ReLU())\n",
    "        self.advantages_layer2 = nn.Linear(512, n_acts)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        z = self.layer1(obs)\n",
    "        z = self.layer2(z)\n",
    "        z = self.layer3(z)\n",
    "        z = z.view(-1, 64 * 10 * 7)\n",
    "        \n",
    "        values = self.value_layer1(z)\n",
    "        values = self.value_layer2(values)\n",
    "        \n",
    "        advantages = self.advantages_layer1(z)\n",
    "        advantages = self.advantages_layer2(advantages)\n",
    "        \n",
    "        advantage_means = torch.mean(advantages, dim=1)\n",
    "        advantages = advantages - advantage_means.view(-1, 1)\n",
    "        \n",
    "        qs = values + advantages\n",
    "        \n",
    "        return qs\n",
    "\n",
    "    def train_on_batch(self, target_model, optimizer, obs, acts, rewards, next_obs, terminals, gamma=0.99):\n",
    "        next_q_values = self.forward(next_obs)\n",
    "        max_next_acts = torch.max(next_q_values, dim=1)[1].detach()\n",
    "        \n",
    "        target_next_q_values = target_model.forward(next_obs)\n",
    "        max_next_q_values = target_next_q_values.gather(index=max_next_acts.view(-1, 1), dim=1)\n",
    "        max_next_q_values = max_next_q_values.view(-1).detach()\n",
    "        \n",
    "        terminal_mods = 1 - terminals\n",
    "        actual_qs = rewards + terminal_mods * gamma * max_next_q_values\n",
    "            \n",
    "        pred_qs = self.forward(obs)\n",
    "        pred_qs = pred_qs.gather(index=acts.view(-1, 1), dim=1).view(-1)\n",
    "        \n",
    "        loss = torch.mean((actual_qs - pred_qs) ** 2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = []\n",
    "        \n",
    "    def add_step(self, step_data):\n",
    "        self.data.append(step_data)\n",
    "        if len(self.data) > self.capacity:\n",
    "            self.data = self.data[-self.capacity:]\n",
    "            \n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.data))\n",
    "        indices = np.random.choice(range(len(self.data)), n, replace=False)\n",
    "        samples = np.asarray(self.data)[indices]\n",
    "        \n",
    "        state_data = torch.tensor(np.stack(samples[:, 0])).float().cuda()\n",
    "        act_data = torch.tensor(np.stack(samples[:, 1])).long().cuda()\n",
    "        reward_data = torch.tensor(np.stack(samples[:, 2])).float().cuda()\n",
    "        next_state_data = torch.tensor(np.stack(samples[:, 3])).float().cuda()\n",
    "        terminal_data = torch.tensor(np.stack(samples[:, 4])).float().cuda()\n",
    "        \n",
    "        return state_data, act_data, reward_data, next_state_data, terminal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100000\n",
    "max_steps = 1000\n",
    "er_capacity = 100000 # 1m in paper\n",
    "n_acts = env.action_space.n\n",
    "train_batch_size = 32\n",
    "learning_rate = 2.5e-4\n",
    "update_freq = 4\n",
    "print_freq = 100\n",
    "frame_skip = 3\n",
    "n_anneal_steps = 1e5 # Anneal over 1m steps in paper\n",
    "target_update_delay = 10000 # How many episodes in between target model update\n",
    "epsilon = lambda step: np.clip(1 - 0.9 * (step/n_anneal_steps), 0.1, 1) # Anneal over 1m steps in paper, 100k here\n",
    "n_step = 4\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #0 | Step #74 | Epsilon 1.00 | Avg. Reward 2.00\n",
      "Episode #100 | Step #7508 | Epsilon 0.93 | Avg. Reward 1.63\n",
      "Episode #200 | Step #14929 | Epsilon 0.87 | Avg. Reward 1.50\n",
      "Episode #300 | Step #22897 | Epsilon 0.79 | Avg. Reward 2.21\n",
      "Episode #400 | Step #30906 | Epsilon 0.72 | Avg. Reward 2.88\n",
      "Episode #500 | Step #39587 | Epsilon 0.64 | Avg. Reward 3.72\n",
      "Episode #600 | Step #50038 | Epsilon 0.55 | Avg. Reward 5.49\n",
      "Episode #700 | Step #61304 | Epsilon 0.45 | Avg. Reward 7.17\n",
      "Episode #800 | Step #74235 | Epsilon 0.33 | Avg. Reward 8.99\n",
      "Episode #900 | Step #88175 | Epsilon 0.21 | Avg. Reward 10.03\n",
      "Episode #1000 | Step #105590 | Epsilon 0.10 | Avg. Reward 16.74\n",
      "Episode #1100 | Step #124976 | Epsilon 0.10 | Avg. Reward 17.95\n",
      "Episode #1200 | Step #144454 | Epsilon 0.10 | Avg. Reward 18.47\n",
      "Episode #1300 | Step #163372 | Epsilon 0.10 | Avg. Reward 18.20\n",
      "Episode #1400 | Step #183653 | Epsilon 0.10 | Avg. Reward 21.62\n",
      "Episode #1500 | Step #204059 | Epsilon 0.10 | Avg. Reward 21.45\n",
      "Episode #1600 | Step #225714 | Epsilon 0.10 | Avg. Reward 23.65\n",
      "Episode #1700 | Step #247110 | Epsilon 0.10 | Avg. Reward 23.41\n",
      "Episode #1800 | Step #268373 | Epsilon 0.10 | Avg. Reward 25.32\n",
      "Episode #1900 | Step #290166 | Epsilon 0.10 | Avg. Reward 24.06\n",
      "Episode #2000 | Step #312644 | Epsilon 0.10 | Avg. Reward 26.02\n",
      "Episode #2100 | Step #335169 | Epsilon 0.10 | Avg. Reward 25.84\n",
      "Episode #2200 | Step #358215 | Epsilon 0.10 | Avg. Reward 28.16\n",
      "Episode #2300 | Step #380483 | Epsilon 0.10 | Avg. Reward 26.99\n",
      "Episode #2400 | Step #403165 | Epsilon 0.10 | Avg. Reward 26.17\n",
      "Episode #2500 | Step #427448 | Epsilon 0.10 | Avg. Reward 27.07\n",
      "Episode #2600 | Step #451457 | Epsilon 0.10 | Avg. Reward 27.91\n",
      "Episode #2700 | Step #474612 | Epsilon 0.10 | Avg. Reward 27.25\n",
      "Episode #2800 | Step #497902 | Epsilon 0.10 | Avg. Reward 27.22\n",
      "Episode #2900 | Step #520962 | Epsilon 0.10 | Avg. Reward 26.29\n",
      "Episode #3000 | Step #544649 | Epsilon 0.10 | Avg. Reward 28.88\n",
      "Episode #3100 | Step #567820 | Epsilon 0.10 | Avg. Reward 29.98\n",
      "Episode #3200 | Step #590991 | Epsilon 0.10 | Avg. Reward 26.73\n",
      "Episode #3300 | Step #613161 | Epsilon 0.10 | Avg. Reward 26.25\n",
      "Episode #3400 | Step #636077 | Epsilon 0.10 | Avg. Reward 27.13\n",
      "Episode #3500 | Step #658902 | Epsilon 0.10 | Avg. Reward 28.48\n",
      "Episode #3600 | Step #681638 | Epsilon 0.10 | Avg. Reward 27.84\n",
      "Episode #3700 | Step #705110 | Epsilon 0.10 | Avg. Reward 27.58\n",
      "Episode #3800 | Step #728200 | Epsilon 0.10 | Avg. Reward 27.24\n",
      "Episode #3900 | Step #752272 | Epsilon 0.10 | Avg. Reward 28.85\n",
      "Episode #4000 | Step #777276 | Epsilon 0.10 | Avg. Reward 32.25\n",
      "Episode #4100 | Step #802479 | Epsilon 0.10 | Avg. Reward 33.16\n",
      "Episode #4200 | Step #826807 | Epsilon 0.10 | Avg. Reward 30.94\n",
      "Episode #4300 | Step #851413 | Epsilon 0.10 | Avg. Reward 32.46\n",
      "Episode #4400 | Step #876090 | Epsilon 0.10 | Avg. Reward 31.96\n",
      "Episode #4500 | Step #900605 | Epsilon 0.10 | Avg. Reward 32.08\n",
      "Episode #4600 | Step #924961 | Epsilon 0.10 | Avg. Reward 29.15\n",
      "Episode #4700 | Step #948415 | Epsilon 0.10 | Avg. Reward 29.69\n",
      "Episode #4800 | Step #973729 | Epsilon 0.10 | Avg. Reward 32.72\n",
      "Episode #4900 | Step #998028 | Epsilon 0.10 | Avg. Reward 32.04\n",
      "Episode #5000 | Step #1022450 | Epsilon 0.10 | Avg. Reward 31.86\n",
      "Episode #5100 | Step #1046545 | Epsilon 0.10 | Avg. Reward 28.37\n",
      "Episode #5200 | Step #1070938 | Epsilon 0.10 | Avg. Reward 32.82\n",
      "Episode #5300 | Step #1094434 | Epsilon 0.10 | Avg. Reward 29.83\n",
      "Episode #5400 | Step #1118830 | Epsilon 0.10 | Avg. Reward 32.26\n",
      "Episode #5500 | Step #1142960 | Epsilon 0.10 | Avg. Reward 32.23\n",
      "Episode #5600 | Step #1166550 | Epsilon 0.10 | Avg. Reward 33.39\n",
      "Episode #5700 | Step #1190757 | Epsilon 0.10 | Avg. Reward 32.02\n",
      "Episode #5800 | Step #1215607 | Epsilon 0.10 | Avg. Reward 34.20\n",
      "Episode #5900 | Step #1239671 | Epsilon 0.10 | Avg. Reward 33.95\n",
      "Episode #6000 | Step #1264839 | Epsilon 0.10 | Avg. Reward 33.32\n",
      "Episode #6100 | Step #1289028 | Epsilon 0.10 | Avg. Reward 33.24\n",
      "Episode #6200 | Step #1312615 | Epsilon 0.10 | Avg. Reward 31.24\n",
      "Episode #6300 | Step #1335572 | Epsilon 0.10 | Avg. Reward 30.97\n",
      "Episode #6400 | Step #1360409 | Epsilon 0.10 | Avg. Reward 33.42\n",
      "Episode #6500 | Step #1385165 | Epsilon 0.10 | Avg. Reward 33.71\n",
      "Episode #6600 | Step #1410852 | Epsilon 0.10 | Avg. Reward 37.12\n",
      "Episode #6700 | Step #1435350 | Epsilon 0.10 | Avg. Reward 33.23\n",
      "Episode #6800 | Step #1461158 | Epsilon 0.10 | Avg. Reward 35.82\n",
      "Episode #6900 | Step #1485897 | Epsilon 0.10 | Avg. Reward 34.49\n",
      "Episode #7000 | Step #1511423 | Epsilon 0.10 | Avg. Reward 33.01\n",
      "Episode #7100 | Step #1536133 | Epsilon 0.10 | Avg. Reward 37.79\n",
      "Episode #7200 | Step #1559917 | Epsilon 0.10 | Avg. Reward 32.11\n",
      "Episode #7300 | Step #1583501 | Epsilon 0.10 | Avg. Reward 32.13\n",
      "Episode #7400 | Step #1607368 | Epsilon 0.10 | Avg. Reward 32.00\n",
      "Episode #7500 | Step #1633010 | Epsilon 0.10 | Avg. Reward 36.64\n",
      "Episode #7600 | Step #1657613 | Epsilon 0.10 | Avg. Reward 34.92\n",
      "Episode #7700 | Step #1682369 | Epsilon 0.10 | Avg. Reward 33.57\n",
      "Episode #7800 | Step #1707531 | Epsilon 0.10 | Avg. Reward 34.24\n",
      "Episode #7900 | Step #1732411 | Epsilon 0.10 | Avg. Reward 35.03\n",
      "Episode #8000 | Step #1756920 | Epsilon 0.10 | Avg. Reward 31.56\n",
      "Episode #8100 | Step #1782425 | Epsilon 0.10 | Avg. Reward 36.53\n",
      "Episode #8200 | Step #1808581 | Epsilon 0.10 | Avg. Reward 35.29\n",
      "Episode #8300 | Step #1833315 | Epsilon 0.10 | Avg. Reward 35.34\n",
      "Episode #8400 | Step #1857188 | Epsilon 0.10 | Avg. Reward 32.65\n",
      "Episode #8500 | Step #1883222 | Epsilon 0.10 | Avg. Reward 40.58\n",
      "Episode #8600 | Step #1908776 | Epsilon 0.10 | Avg. Reward 36.95\n",
      "Episode #8700 | Step #1932769 | Epsilon 0.10 | Avg. Reward 32.62\n",
      "Episode #8800 | Step #1957555 | Epsilon 0.10 | Avg. Reward 35.38\n",
      "Episode #8900 | Step #1983506 | Epsilon 0.10 | Avg. Reward 38.20\n",
      "Episode #9000 | Step #2011309 | Epsilon 0.10 | Avg. Reward 39.44\n",
      "Episode #9100 | Step #2035503 | Epsilon 0.10 | Avg. Reward 33.27\n",
      "Episode #9200 | Step #2060982 | Epsilon 0.10 | Avg. Reward 41.85\n",
      "Episode #9300 | Step #2087063 | Epsilon 0.10 | Avg. Reward 40.09\n",
      "Episode #9400 | Step #2112582 | Epsilon 0.10 | Avg. Reward 38.61\n",
      "Episode #9500 | Step #2138959 | Epsilon 0.10 | Avg. Reward 39.64\n",
      "Episode #9600 | Step #2163859 | Epsilon 0.10 | Avg. Reward 37.04\n",
      "Episode #9700 | Step #2189015 | Epsilon 0.10 | Avg. Reward 36.90\n",
      "Episode #9800 | Step #2216385 | Epsilon 0.10 | Avg. Reward 49.73\n",
      "Episode #9900 | Step #2240576 | Epsilon 0.10 | Avg. Reward 35.72\n",
      "Episode #10000 | Step #2265176 | Epsilon 0.10 | Avg. Reward 38.72\n",
      "Episode #10100 | Step #2289779 | Epsilon 0.10 | Avg. Reward 37.19\n",
      "Episode #10200 | Step #2314158 | Epsilon 0.10 | Avg. Reward 39.72\n",
      "Episode #10300 | Step #2338706 | Epsilon 0.10 | Avg. Reward 41.53\n",
      "Episode #10400 | Step #2362156 | Epsilon 0.10 | Avg. Reward 39.94\n",
      "Episode #10500 | Step #2388164 | Epsilon 0.10 | Avg. Reward 37.35\n",
      "Episode #10600 | Step #2415413 | Epsilon 0.10 | Avg. Reward 32.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bfda5ddd6390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mobs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             model.train_on_batch(target_model, optimizer, obs_data, act_data,\n\u001b[1;32m     68\u001b[0m                                  reward_data, next_obs_data, terminal_data)\n",
      "\u001b[0;32m<ipython-input-5-dc1b9234e6f1>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "er = ExperienceReplay(er_capacity)\n",
    "model = DQN(n_acts=env.action_space.n).cuda()\n",
    "target_model = copy.deepcopy(model)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, eps=1e-6)\n",
    "all_rewards = []\n",
    "global_step = 0\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    n_step_rewards = []\n",
    "    n_step_states = []\n",
    "    n_step_actions = []\n",
    "    prev_frames = []\n",
    "    obs, prev_frames = preprocess_obs(env.reset(), prev_frames)\n",
    "    n_step_states.append(obs)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "\n",
    "        ### Enact a step ###\n",
    "        \n",
    "        if np.random.rand() < epsilon(global_step):\n",
    "            act = np.random.choice(range(n_acts))\n",
    "        else:\n",
    "            obs_tensor = torch.tensor([obs]).float().cuda()\n",
    "            q_values = model(obs_tensor)[0]\n",
    "            q_values = q_values.cpu().detach().numpy()\n",
    "            act = np.argmax(q_values)\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            next_obs, reward, done, _ = env.step(act)\n",
    "            cumulative_reward += reward\n",
    "            if done or step >= max_steps:\n",
    "                break\n",
    "        episode_reward += cumulative_reward\n",
    "        reward = format_reward(cumulative_reward)\n",
    "\n",
    "        next_obs, prev_frames = preprocess_obs(next_obs, prev_frames)\n",
    "\n",
    "        ### N-step Calculations ###\n",
    "    \n",
    "        n_step_rewards.append(reward)\n",
    "        n_step_rewards = n_step_rewards[-n_step:]\n",
    "        n_step_actions.append(act)\n",
    "        n_step_actions = n_step_actions[-n_step:]\n",
    "        \n",
    "        if len(n_step_rewards) == n_step and len(n_step_states) == n_step and \\\n",
    "           len(n_step_actions) == n_step:\n",
    "            ns_obs = n_step_states[0]\n",
    "            ns_act = n_step_actions[0]\n",
    "            ns_reward = sum([reward * gamma ** i for i, reward in enumerate(n_step_rewards)])\n",
    "            ns_next_obs = next_obs\n",
    "            ns_done = int(done)\n",
    "        \n",
    "            er.add_step([ns_obs, ns_act, ns_reward, ns_next_obs, ns_done])\n",
    "            \n",
    "        n_step_states.append(next_obs)\n",
    "        n_step_states = n_step_states[-n_step:]\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        ### Train on a minibatch ###\n",
    "        \n",
    "        if global_step % update_freq == 0 and len(er.data) > train_batch_size:\n",
    "            obs_data, act_data, reward_data, next_obs_data, terminal_data = er.sample(train_batch_size)\n",
    "            model.train_on_batch(target_model, optimizer, obs_data, act_data,\n",
    "                                 reward_data, next_obs_data, terminal_data)\n",
    "        \n",
    "        ### Update target network ###\n",
    "        \n",
    "        if global_step and global_step % target_update_delay == 0:\n",
    "            target_model = copy.deepcopy(model)\n",
    "        \n",
    "        ### Finish the step ###\n",
    "        \n",
    "        step += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if len(n_step_rewards) == n_step and len(n_step_states) == n_step and \\\n",
    "       len(n_step_actions) == n_step:\n",
    "        for i in range(1, n_step):\n",
    "            ns_obs = n_step_states[i-1]\n",
    "            ns_act = n_step_actions[i]\n",
    "            ns_reward = sum([reward * gamma ** i for i, reward in enumerate(n_step_rewards[i:])])\n",
    "            ns_next_obs = next_obs\n",
    "            ns_done = 1\n",
    "            \n",
    "            er.add_step([ns_obs, ns_act, ns_reward, ns_next_obs, ns_done])\n",
    "            \n",
    "    all_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % print_freq == 0:\n",
    "        print('Episode #{} | Step #{} | Epsilon {:.2f} | Avg. Reward {:.2f}'.format(\n",
    "            episode, global_step, epsilon(global_step), np.mean(all_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode #0 | Step #75 | Epsilon 1.00 | Avg. Reward 1.00\n",
    "# Episode #100 | Step #6985 | Epsilon 0.94 | Avg. Reward 1.11\n",
    "# Episode #200 | Step #14440 | Epsilon 0.87 | Avg. Reward 1.55\n",
    "# Episode #300 | Step #22300 | Epsilon 0.80 | Avg. Reward 2.16\n",
    "# Episode #400 | Step #29814 | Epsilon 0.73 | Avg. Reward 2.43\n",
    "# Episode #500 | Step #38500 | Epsilon 0.65 | Avg. Reward 3.50\n",
    "# Episode #600 | Step #48150 | Epsilon 0.57 | Avg. Reward 4.79\n",
    "# Episode #700 | Step #58476 | Epsilon 0.47 | Avg. Reward 5.28\n",
    "# Episode #800 | Step #70518 | Epsilon 0.37 | Avg. Reward 7.62\n",
    "# Episode #900 | Step #83728 | Epsilon 0.25 | Avg. Reward 9.20\n",
    "# Episode #1000 | Step #99126 | Epsilon 0.11 | Avg. Reward 12.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_frames = []\n",
    "obs, prev_frames = preprocess_obs(env.reset(), prev_frames)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    if np.random.rand() < 0.00:\n",
    "        act = np.random.choice(range(n_acts))\n",
    "    else:\n",
    "        obs_tensor = torch.tensor([obs]).float().cuda()\n",
    "        q_values = model(obs_tensor)[0]\n",
    "        q_values = q_values.cpu().detach().numpy()\n",
    "        act = np.argmax(q_values)\n",
    "\n",
    "    for _ in range(frame_skip):\n",
    "        next_obs, reward, done, _ = env.step(act)\n",
    "        if done or step >= max_steps:\n",
    "            break\n",
    "            \n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    obs, prev_frames = preprocess_obs(next_obs, prev_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efb44120310>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU1fnA8e+ZyUYgLIGwgwFBERAEIoKACyAuuO/WhVZbW1tt1VaL9VeX1q3W1tbWaqlL1VqXqi0qLiCCCyKbgoDsGGQn7CEh+/n9cZe5d+ZOZpLMksm8n+fJkzt37sycm4F3zpz7nvcorTVCCCFSjy/ZDRBCCNE4EsCFECJFSQAXQogUJQFcCCFSlARwIYRIURmJfLFOnTrpwsLCRL6kEEKkvCVLluzWWhcE709oAC8sLGTx4sWJfEkhhEh5SqlNXvtlCEUIIVKUBHAhhEhREsCFECJFSQAXQogUJQFcCCFSVFRZKEqpYqAUqAVqtNZFSql84BWgECgGLtVa74tPM4UQQgRrSA/8VK31cVrrIvP2VGC21ro/MNu8LYQQIkGaMoRyHvCcuf0ccH7TmyOEEImzuHgvq7YfTHYzGi3aAK6BmUqpJUqp6819XbTW2wHM353j0UAhhIiXi5+cz5l//iTZzWi0aGdijtFab1NKdQZmKaVWR/sCZsC/HqB3796NaKIQQggvUfXAtdbbzN+7gP8CI4GdSqluAObvXWEeO01rXaS1LiooCJnKL4QQopEiBnClVGulVJ61DUwCVgBvAlPMw6YA0+PVSCGEiLV73lyZ7CY0WTRDKF2A/yqlrOP/rbV+Tym1CHhVKXUd8C1wSfyaKYQQsfXPz4qT3YQmixjAtdYbgaEe+/cAE+LRKCGEEJHJTEwhRNrZsq882U2ICQngQoi0c/BwTbKbEBMSwIUQaadtq4SuZRM3EsCFEGlH62S3IDYkgAsh0k5dC4ngEsCFEGmnzozfPdq3wsiQTk0SwIUQaUebPfAMv0rp4RQJ4EKItGP1wP2p3P1GArgQIg1ZPXCfT7lupxoJ4EKItPO/pVsBMON3yg6jSAAXQqSdx+dsAMBnDqGkaPyWAC6ESF92AE/RLrgEcCFEWqmurbO395VXAdIDF0KIlFBbFwjXWRlGCEzRDrgEcCFEenHOwuzerhUAuoF98Flf72THgYqYtqsxJIALIdKKowNOmxyjqFVDe+A/eH4xZz2W/MWQJYALIdKKswc+rHf7Rj/P3rKqWDSnSSSACyHSijavYd519kAUVhZKlI/VmoMV1XFqWcNJABdCpBWrB64UdiGraMfAX/h8E0PumRmvpjWYBHAhRFqxArhPWf3v6Hvg//p8k+f+A+XJ6ZVLABdCpBUrVvsULNuyH4DSiuiWWCurrA3ZN3PlDob+ZiYLv9kbqyZGTQK4ECKtBIZQFO8s3wEEAnm0j7VUVNcyf+MeAL6K8jliSQK4ECKtWDHY5ygl65zcU58pJxa6bg/49XuxalajSAAXQqSVwBg49Mo3JvK0zo5ukePcLH/c2tUYEsCFEGmlztEDv/vsQQC0zWn8KvXPzisGjCGZRJMALoRIeTW1dYz/w1xmrtwR8dg6K4IryDRroUQ5gtLsSAAXQqScwqkzKJw6g0/WlQDw8Ptr2FhSxq2vLov4WOcYeGBBh8gR/PJp87lr+sqw96/cdiDhZWklgAshUsqSTfvs7aufXgjAtI83AnCosobCqTO47p+Lwj7emrTjU4ELmdH0wD/fWH+a4BtfbOU/i7dEfqIYkgAuhEgp/zCDdX1mr94V9j7nGLiy98Wm57yh5FBMnidaEsCFECllbP9O9nb/zm0AuGBYj5DjPt+4x3NIwz2VvmG1UCLJzkhsSJUALoRIiL1lVVTWhM5kbKge7VvZ293M7S5tc0KOu3za5/bQipN2TKVvyBh4NHKjTEeMFQngQoi4q66tY/hvZ/GTF79s0OPKKmuY8dV2176t+w/b2yN6dwCgtq4OL4/PWR+yz05CUeDzRT8GHo1XFm1O6IXMqD8ulFJ+YDGwVWt9tlKqD/AykA98AVyttU5+gVwhRLMz/LezAPhg1c6oH3PFtM/taertc09gTD9j6OT//rfCPsanYF9ZlT0lPthBjxonFdXGt4DsDL/dA4/VGPg3u8vYuLuMIwvaxOT5ImlID/xnwCrH7d8Bj2qt+wP7gOti2TAhRMsRbbEoJyt4A1z51ALPY6rrNMN+O8vVKw924LC7UmB1rRGsM/0KsHrgses1+xM4oSeqAK6U6glMBp4ybytgPPCaechzwPnxaKAQIrXFc0ghmjH1Fxe4S8DWmKvSZ/p9gTHwoMdorZm+dKtrBftofbu3vMGPaaxoh1D+BNwO5Jm3OwL7tdbWx+oWIPQyMKCUuh64HqB3796Nb6kQIiV9vG53gx9TVeMOnHnmxcFDle6e/Pb97oWFfSp0PLsuaIdVuCrDp+w8cOeHTPHuMk55ZC4AG0vKuOW0oxrU9sYE/caK2ANXSp0N7NJaL3Hu9jjU82NWaz1Na12ktS4qKChoZDOFEPWpqa3jo7UlyW6GpynPLGzwYx79YK3r9ohC42JldVBgf3PZNtdtr4uRe4LWrqy2Arg/EMCd10Bvey0wm/PPs9fZY+aW/NZZ9ba9a7vQjJh4iWYIZQxwrlKqGOOi5XiMHnl7pZTVg+8JbPN+uBAi3p6Yu4EpzyxstkHcqXDqDHsYw8ueQ5Uh6X+d2mQDUB0m26Q+W/e5x8et187w+ewl1awx8No6zaLifa7jdx2sdN3+8+XH1ft6iZxNHzGAa63v0Fr31FoXApcDH2qtrwTmABebh00BpsetlUKIen2zuwyAktLKCEc2D3vLvRPW1u0sZcR9H4TU5z5kXgStqW14dJx4TBcANu8tZ9bXO6lx9MADAdz4fcxdofW9d5a6h2mKjsjn7ZvGhn294B57PDUl6/yXwMtKqfuAL4GnY9MkIURDWWEt8QVNGydcIA7u/Vr2H66q93H1sR477uE5rv0ZPp/d8/50fQlnDO4aMvYOsOOAEcCH9GyHUopWWX4G92gX9vUufnI+xQ9NbnA7G6NBE3m01nO11meb2xu11iO11v201pdorVPjo1+IFmi/2aMNHhNOtuAUPku4QJzh8/4IOlxtBNaqMEMv7Vplhuxbce/pADzwzmo+2xB6IdXvuIj5r8+/ZU6Y+ik3vWRMPlJKuV6nsGOu5/GJJDMxhWgBdh8yAnhzGwOvDDOcsC/MEMrUN76q93lqwoyB33f+YP5yxTDXvhxHXRKvBYd9yj3c8XbQjE9L34LWxobWrm84v7toiOfxiSQBXIgUs35XKec/Ps+VUmcVeLpqVPNK1Q034HHe4/M894eb0r56RykA1TXeB4zsk0/r7MByZ49cMpQMfyC8je7bMeQxSinXWPzrX3iXgj2pfwEV1bUs23KAw46An52Z/OXVJIAL0YyUVlRHXN38d++tYenm/XzqyK+2Mh8OHm74jMd48hpTthROncGuoAuEkVhB9qfj+/GrswbY+9vmZOL3BcLZhWZ1wvOO684RHXNdwdxSU1vHyf3rT21u1yoTrTX//KwYcPfkrcqDIwvzWXTnRB5zfAMIzj2PFwngQjQjP3h+Mef+dV69gc+vQqd/W0MBjR0D/2RdCb956+sGZ7EcrKgOO84NgTHrkX3yPe9/6pNv6n3+K0b2sre/2V1mB9Je+bkc1SXPvk8paGNO9snwKbtIlV8p6rRms8fsyN2HquzjwvEp41uE1xDMgK553Hf+YP5xTREFedl0yA2Mj5dVJeaDVAK4EM2ItepLRT1TxP1m0HGm2jW1TOvVTy/kmXnfMOahDxv0uCH3zGTovTPD3r/zoNHDPu+47p73d4wwKebBCwPjzH+YucbezvT7XIsIKwWd84xc8QLzt7FfUVcHN7+yNOS5W5krzF9W1CvkPuO1j0UpRVVNHcN7tw+5XynFVaOOoJ0ZuJ2d7uA0yHiRAC5EM7RupzHmq7WmcOoMiu77wL4vw28ELucFvZcWbgbgmG5tG/xam/aU2dvhsjwiWfjNXqpq6pj28QbXt4d5641hnm37D/PLMwaEPM5raMNy7lAj6N977iDXbTAXY3Acq1BkZxrP5Xx9v8/9TeX7Y/vY21bnO9yHyxUje7O3rIqXF21m5teRqyg6XydRk3kkgAvRDF30xHwA5qwxUtt2HwoMbVi92ifnbqS6to7RD8627+vaNptI7njjKwqnzrDXlgxX6S8SZ5GoS/8+nynPLOSBd1bz8Hur7f29841UuytG9mZIz9Dc6foGMLqY53JCX2P4paZOk2UG/DH9OuEs+uf3KbIzjB6180NoY0kZ2w8Extn7WBklzjZEUT3wqy0HIh6T6AWNQQK4EM2aswzromJjeMX6dr5mZyn973zXFaCimedi9dYveuIzALbsC1+KNZyyyhru/O8K1z6r/Kvz+arMBmVn+D0D+KuLN7tuTzyms73dJtsYmsgxA/PCb/ZyXK/2jOqbT6c22a6iUX6fItccErnVUXxq8Sb3xCDnyIa1IqY/wji4079/cELY+6odf/xEhXIJ4EI0Y5WO4YBLnpzPnDW7OPmo8JkTscx+eGvZNnsIpL52Wc43hyImOIJwrV13RJGXk8miOye6HmOlB4LRg/1glfGN48oTevP9ccZwhzVW/c/Pivl2b7kdcOesdue8Z/p9FD80me+N6UM4FVWh1woaEL858chOYe9bvb007H3xIgFciCSqqqnjrukrwmZ/dA1a6/F7zy6y09cmDOjsum9A17yYLExg9WxveunLsMMryzxSHa1XtsboAbvuiJXtUZCXzT+uKbLvH+rolT8/PzAkc/8Fx9LazCpp78ju2HGwwk4XPOvYblGfk6W1x5qVkTJRovXTCf3s7UQNp0gAFyKJZq/ayfPzN3HvWyuBQN3r+lgTeDKDLgC2bZXZoOyHfuaK7sGvueOAOzf7neXbeXe5e5bivxd8G/J805caKYy3vBIox+qsvW05bWAXe/u4XkZ2x67SCu5+c6VnO62xbYv1+TCqbz6ZfsUtE6Ov1z2kZztuOOVITuiTz6Dubc3niy6AX368d7aKRSnFb84zLrgmagglsUsoCyFcrHhr1QYpDVqwwGtxAGv4wjFvhaIjOuBXynOqeV2dpu+v3uEXk47i2J6BdLj1uw4BEBy/MvzKlcnx4xe/AHAVaLLKu4Zz6d/n8+oPR1Nr9kTDjTNbY+SHPYY2wrF64Eop1t1/VtSPy8vOYHCPdiGFqHxRBvBtByJPOkp0MTHpgQvRQGt3llI4dUbEGZPRsKZyf7ZhN4VTZ7ju++Lbfa4LlJZKs7CTc/z4++P64vcpzx74tgPGRcVHZq71XFyhorqOH518JL+/2Mi5rqnVlFWGTkRxPvfIPh3qPS9r4kttbWgPHOC9m8cBgQ+oaIMoGKmB0cp21EMJ/nC0OD8IczLDP/nHUdSZsRZRfm+F9yLLsSYBXCTM1v2HeeCdVfUW808Fkx79GIBrPILhkk17uf21ZVGf46/NFda9Vk+/8G+fuVZgt7y2xMjc2FgSyN+u0xqfT3lmoXjNzjyyoDUdco1p4lW1dWRl+Mgyg11VbV3I0mXgnixUHUW6i9baHgMP7oEP6NqWIzrm2n+nhgz9ZPiiD1szbzkp4jGrHBcf5/7i1Kif24s149PrfYsHCeAiYe57+2umfbyRzzbsiXxwM6C1ZuW28Pm/+8tDp5Bf9MR8Xl28he8/vzhu7fIK9hk+hV95Z6H06mDkYh/VpY29r3NeDhXVdXYgzvIre0z9hfmbQmpng9FTt0RTl7uypo7aOo1PeedaZ/iU/fq1jot+t51+dL3P25CLjkd0DOR9f3K7d3A+6ahAZomVe+6cwt8QSzc3/VtZQ0gAFwljjd16paA1R9OXbmPyY5+6vg5vKDnkOkZrbQ+pvOXo6c5dE/nrdrheeo/2rRrc1hP6dAw7hGKNL6/dGWh7bpafippae9JLVobPniRj1RsJ9svXjVKvWmueM4+ZVU8Pt6yyhlqtw/aYM/0++/WdHzwXDe8Zcuyvzx5ob4erGR7OsrsmMfvnJ9Mr37t+d8fW2RzTrS1/v3oESimKH5rsmsJvuWl8P49HuzmHtQqnzoj7AscSwEXCfGgWzF/uMXY8felWHnxnVcj+/eVVnPL7OfX2hOPllUXGUIW1XBkYxaac+tzxjj2kct+Mrxv0/Ps8evAAndq464P079zG8zindrmZVNbU8fX2gyEfMve8FZrdUZCXjdbYY92Zfh+ZGfWHg1nmdPK731zJGnOqf0Fedkiqo6W8qpaa2jrCjXis3lFqP6c11PLElcM9FwW+tCgQ1BsyXg7G3+bIgvB/Q79P8e7PxnH6oK71Ps8wj3oowV78vnuijzX5Kl4kgIuEe+zD9SH7fvbyUv4etJAtwLz1eyjeU85fZoc+Jt6sqeb5rY08ZK21a9w5WFaEABjMqncS7PqTjnTdbpOTYU+S8WJdePvELC874Q8fue4v98jw6G9W8jtoVhLMyvCR6Y8cGOvqNO8sD3wjyfD7eGpKkeuYJ64cDhjpjtW1genv9T3nywtD0xKdnCmTDe2BN8WArs6Kh5Ffd0w/90Sf4BTIWJMALpo16+LXwYrwJUvjJddcIKBdq0yWbNrHjoP1p5Ft3uuekn7HG8vrPf5Gc6muYEWF7gyPtjmZ/Oly92ozzkk8zrHpaEy7egStzMUIrPH0TL/PlbERzuHqWlddllaZxvqQf7rMWKn9iSuHk2M+d2VNHQcrqiMOmT0/v5jnzEk8SzZ5r4npDOCxmngTjfduPolx5mIZjXnVeFcllAAuEi54BmF9rACejAufI3obgfRH//qCi574jNEPNqzU6kthepVLNu1j0F3vsbcsdFmxX589kLwc9/QMa6UZZwfw6e8eX+9r15cF0719K7btNz5sfvay8SGS5feFTAwCowd6rCNv+py/fOq633p/zh/Wg+KHJnPmsd3sALu7tJI3vtgaMYDf81Zg6Onr7Qc9j/H7lN3zTmQPHODy441VjgZ2j67S4zmOqolNLfMbiQRwEXdaa1da2uEw6yRaxzrV91+1cOoMpr7uvYYiGDMKI01p3ry3nNU73EHjs/W7+cmLX9Tb04t29l+5R2H/i574jDKPYY21953JdWP7kJuVwce3BTImrCGL9Q2YtNLvzncBY4WfYDmZPjsYWYWnjCGU0HDw3s0n8eaNY+zbG3eHH0KyLDbHfRuTidPXo1qgxZqi35DiU7EweUg3ih+aTOc877H+YI9cMsRen7Oh344aSgK4iLtn5xUz+O737dtllTWegQ1C84u/9VhJBWCf2Xt9edFmz/sXbNzDqAdn8/Sn4Vd82bSnjHEPz+GMP33i2v+dpxYwY/n2er/+1vch5DTwrsB5byw5FDJZB+DoLnlcO6aPawzdub6jJVzg+umE/mFf38qgmeSYvp6d4adDrvtCaWaYHjgYY7//N/kY+7ZVc3zNfWd4Hn9pmAUSgp3hcdGwvhmeVjBsTPXERMrO8Nt/o2j/nTSWBHARd795252dsWzLAQbe9b4dIJ2Bcn/QauXBj7WUO/5jFE6dwQvzi+3bByuq7cD++Bz3xc+1O0vtr7Un/36u6zmChZvefWlRTy4a3sPzPjAq6XkJtyjAGz8+kbvOGejaF83IqbXyzIqt4TN0bnvN+IbizE/2+xRZGe4Pg6yM+sfAncMWq8xhjnAX6Lp5ZJF4+eHJfUP2De0VOdMj3sMSsWCVtj0c56XVJICLpDl4uJpDlTUc+at37H3PhslBBqMHa/nfl1td9/16eiBVbsg9M/mvef+pjvH2XaUVTHr0Y37zVnTpflZ962Bd2+aEHdd968axIcHYcnyh9/Rzr561sxbJtY7yqD+d0J+nzayPK0YaHxRWKmBhx0Ce8+mDAj1ugBOPDKzKXqd1SCpepmMiD8BjVwxzTXzJzYq+bFKG31fvlHTLsN4dXJkvlxX14tSjI18fyWkGq8FH0tr8e70b5yn1EsBF3J052Du/dk9ZFdf+c5Fr3xNzN/CX2es4VFnD9KXuIO1Mh/v9+2sItmLrgZDeqLN3/y8z0+FFj0p6YFz4C5fa5/SDk/q6lsyyxog75GZybM92ZGf4PWt2h7uu6JXX7OzF3nJaYIjk1tOOYsIxRnA+wpyY0t2c+PPKD0fz+g2jKeyYG9I7vtkxZp/p9zGou7ugU3ZQGuExXfNcE1/OGxY+jdGLtRgDYGdxeHEOmfXoEN0EprY5mZEPSrK2rYwAPndNCWt2xK9OuARwEXcdwixce7iqli+/DU0b+8OstQy++/2Q8WuvfGans//yKWcHZUlYvcp1O0vt/HO7jGhQz3fOmhJOMyflOP3x0qGu222yM+wa1VeN6s2Qnu35/I4JzHVceHQurGsJN6bulX/tzDnOCxOwupgTaKyysF3a5jDiiHyK95SH1D/p7QjGXdrmkJXhc60FueNApWsiT/BnSkPzmZ2phi9cF34VG6dwY/DBjumWF/mgJHO+f8/PL2bSox9RHMUF4IaSAC7izlnZzjlr79u95fUWReoSNMPPOT5+zegjonptayrz5n2Bi6HWBby+ndwZD+HGVi8MmtqtlKJXfi5v3zSWu8426j93bZdDu1aBQPvryYFhFGsNS2uxBec3krycjLATRGb//GTX4gfBxvbvxL9/cAI/OvnIsMdYvDJqtjouBiqFnRsO3h+Wz107MuLrNNSYfoGhnUgToT795alcMKyHa0gpFWzaU87anYfsRZdjSQK4iLuyyhpyMn2MOKIDM28N1M74yb+/sLffvmlsyOOsHqsV365/YYl932xz6a1oXhtg1teB463nK6+q5Zyh3RnZx1g0d5lHIaKOYb49AAzu0S5s0GnnWEXmhAeMRYet6eLWUmEAt9dTuOnIgjauxQ+8nHhkp4hpdeHGo511Okb17ejqAXf3qMfiHBbq0yl8ul9DPHDBsfZ2VoSZoD075PLoZccldCJPLFilDYIzf2JBAriIu0OVNQzp0Z7XbziRtjmZPOXRqwwusg+B2im/mBQa5Lbujy6VzFpjcXCPwCQMa/bh1v2HaZOdwcMXGYWL/vFJYMjmqC5t+O6JhXz481MAeO1Ho41j6ukRR7LbXDbN7/Nxx5kDgNBvGbFgTSSprdMM6dmO0X07eh63zLHSen7QB5WVRRHOf8y/R1Md0bG1nU7Y0FIEqcKq6R6Pi68t8y8mmpWyylpXXrNz0VuAEUfUvzjA9SeFpptZ/hXl+Kqz2t2yzfvtCS4vLfzWM2Vv5i0nc8+5g+yedFFhPsUPTY7YI3Ya1Tff3t5bVsXP/2MsNVZSWsm1Y/vw1DVFDXq+aA3uHshB9so4iUZOhDHv4Nmi4dx1tndGjpM234Fox8BTRTSlCZqqZf3FRLNUVlnjWkw2eMz35etHAfDCdSM9v5oH/8d2ptiN7d+JZ78Xflp5TzOzIXhCxR9mrgWMSTTO9DuAO886hlh41KwPAjB3TWAIp7Simky/j4kDu0RVIKmhMsy/18qtB6irC/y937pxLO/+bJx9nLPCX7BwwxRPTyni5KMKIhaosvSpZ2al5f2VRn78Fx4XtFPZgl9NiPtrSAAXcbevvMrOi7U4a15bAXpc/wLm/OIUz+c4fVAXjjYr6L24YJPrvlOP7hxygc3q+W3ZdxitdciUZqvm9ZWjeocEUecYdVM4L9jOckziuWBY+ElAsfDUJ0ZVx8umfW72wI39x/ZsZ88QBHjIo+Z1JBOO6cJz146M+MHz3s3jGNe/U9jhGydroYn2rWI/RpxM4bKHYiliAFdK5SilFiqllimlViql7jX391FKLVBKrVNKvaKUall//RZs5bYDLCrey/pdhxq0mGxjvLt8O/vKq3llsXvK+/v1LARg1ZFwysn021kiXqvBnBSUa3zt2EAQrq7VnoWjwPtrbqx6xc7ncU7oiEev2+mqUYEMndU7SglXEcDnUyy6cyKf3xHoKX5w68n8s55vNNEa0LUtL1x3QlTjvmP7GRdHg+ugpzq/T3HWsfXXGG+qaHrglcB4rfVQ4DjgDKXUKOB3wKNa6/7APuC6+DVTxNLkxz7lkifnM/GPH3H9C/Fb+gvghhe/8NzfJjuDD249mddvCL0YdlL/QLaDlaWQneGze9HWhI/JQ7rZxymlXAWgAK4zg3hZZY3d4756lDv9MHh4Jlx9j8b630/GuG539sgPj7VWQUHzg1XeU/jBXJDBMWmoX+c2nBLFbMhY+umEflxW1ItLj2/cMmbN2d+uHAEYa5DGQ8QArg3WHOZM80cD44HXzP3PAefHpYUirj5ZtzvqjI6mCA6cYASLEUfkh+xvl5tpp/pZOdPZGYEe+I/ND4XzhrpnBzrzsCEwmabEMank8qC1Dq10xLX3ncn6+8+MeQH+44Jqe+wqrQxzZOx4LUnWnLXPzeJ3Fw9p0HT9VPLaj0bz6g9jk7UTLKoxcKWUXym1FNgFzAI2APu11tYMjS2A58CeUup6pdRipdTikpLI6wSKxNuw61Dkgxpo054yVynTaKdJW9b89kw+uu0UexZnbpY/pARrcO85eOmucjMHfHHxPvp2as2Arnkc09Vd07lnvtGurAyfffEv1bXLzeSH9WTuiMQqKsynYz1VFpsiqn+xWutarfVxQE9gJOB1md5zpE1rPU1rXaS1LiooCK0PIRLrgMc6jNGsGvLtnvKwJWC9nPz7uRx7z0z79oUNvHCXleFzrSheVVtHVU0dJY4ebHCNDav3fOOpxuKzJ5kTT3713+Vs3F3G6h2lIdkVPxgX/0BntSeRnGPP9ZWbFamtQd9ZtNb7lVJzgVFAe6VUhtkL7wlsq/fBosG27T/MwYpqBnSNbiWQaGzcHdrb3lfufYHP6aTfz2Foz3Ys23KAp64pYmI9+cteCxB3buKElWfnFQNwwd/m2fuCe8xZGT42PHCWnXURzVfyNtnx/9p+fJ98mGNsP3jhsfUfHCPOAD62X/hiUiK1RZOFUqCUam9utwImAqsw/klebB42BZger0amqxMf+jBksYGm8qqMZs0UC2fTHqMIjzVz7/vPL6auTvPu8u28umgz5z0+z3X85Mc+DXmOWIlUzN/vU3aWR6ug2YStzdtWYfyin8IAABUJSURBVKdbTzsqIaVJnVPQ27dKTCW9Vo7p8+HK2IrUF80QSjdgjlLqK2ARMEtr/TbwS+BWpdR6oCPwdPyamX6cdSoiLQvWEE9+tAGAmbecxDcPGkt0/f79Na7JMcG2egTN2177ihte/ILbX/+KZZv32+2ti9MirlZt6h4eNTrCCQ6WU83p6/3N6n0Du8Xum020xtRTWjWWnNcL4p22KJInmiyUr7TWw7TWQ7TWg7XWvzH3b9Raj9Ra99NaX6K1jv/l9TTS31zTEALLYoXz8dqSqFcp6VtgBK9+BW1c/7F/9K8l4R7ieXEveOUcqw73eytD2zq0Z2idk4ayprRbGTPR1CRpn5vpSjX0m1c5f3nmAIb2bMeoIyNPMom1RNWy9qqXLlqelnHZvYUJXs186hvLwx67esdBrnlmITf9+0vX/qP+713O+6t7KENrbReICr6Y9+HqXSFB2fK5x8o0s1e7qwFaE4LW7QwdY3/iqhFh2x+t4MAXTT61UsouGgXGUmsAR3XJY/qNYxMy/m0pfmgyxQ9NTtjrDe8deWkykfokgDdDdwQFbGfh/WA7zPFr53qLW/aVU1VTx7ItB5jp6BH/9OWl9b5uuNmKf5y11t4OFzi/89QCHnl/jSvYn3dcd1bee7pnadKmirZynXNSy/o4pEs2V9EuoiBSmwTwFPD2V9vt7XveXEnh1BmUlFaitea7z7qXJKusqWXs7+bYt501tN8yV2kJl5Xwv6XbeH5+cdgx93X3n8nd5wxy7XvkksBqNX+ds95eR/K35w3iz5cPcxWxiqVoK9d1bJPNk1cN56LhPbn/gsFxaUtz1Do7g8E92qbc4geiYVrm1KcWxtkztqaEH3//ByHH/ePjjfzlw3Uh+3cerHDVnf50/W57+7UfjebiJ+cD8NjsdfbrOddQtGT6fdTUBS525rfOonuYFcjjMR17YLe2fG2uiN4QZwzuxhmDu0U+sIV5+6ZxkQ8SKU164M1IRXUta3aUkpvl5zsn9ObecwO93c17y+t5pOH+d1bZixU4nfDAbOY5grbTiCM68LuL3LnJyx2F/oOHHcY4eu83je/HqDDV5uIxbLLWseBwpFVohEgHEsCbkR++sITT//Qx5VW1dMnLYcqJhfZ94x6eQ+HUGY1+7iufWmBv3zQ+MDNQKcVlx/d2HTtpUGCSzsQ/fgQECvjnO5aFOr4wH59P8eNTQtdkjEeAdQ7sxGpJLyFSmQTwZuSjtYFaMdvMdDmvIlDBvFLqCjvmsvLe0z2P/7nHEmXOnOgXPjfqbTtLzVozCH0+xUe3ncIPxvWxa0tfNCIxxZPmhqkVLkS6kgCeQAcOV7sKPNXWaZ759BsqqkNzuPeY496/PT/yhbdTji7ghqBe8NzbTvW8gHjhcO+aJM6x5RVbje2t+wPDNoWOuiRHdGzNnZMH2r3svp1ac9vpRzNv6ngmHtMlpC2x0is/N/JBQqQRCeAJorVm6L0zXQWenp33Db95+2v++uH6kOPvcwTuAV3z6n3uTL+PX54RyHd25jevv/9M17ETBnjXMOkdFBwLp87g0Q/W2c/nteiwRSnFT07tR4/2rXhqSpGrLUKI+JEslASZ7zEZ5r4ZqwBjMk5w6p4z3/qKkb25+82V9u1bTzsKnzIC57GOwHrf+YP5v/+t4GLHkEaG38fYfp3szBPnzESnd342jjU7DnLRE/PtfZ+uMx7z4c9Pjvo84235PZO8y14KkYYkgCfItv3uglEHDgeGUj5YtYt56wMB/rsnFrpmSl44vIcrgCvgxvGhJUIvO74Xe8uq7JVoLH+/egSD7n6f8QPCp/a1yc4IWVzBamNTKwnGUiLWGRQiVUgAT5Bf/GeZvb1y2wEen+MeNrnqaSNL5JaJR/Gzie7gHLwg8OAwtUUy/T7P2s+tszOinsatFMSwdpYQIo5kDDwJJj/2KZ3CrNAxqm/oEmNWb3xQ97a8fdNYTo3jmoVL75ok2R5CpAjpgSeAVanP6fn5Rqpepl9R7VhlfWSf0AAORjnV/NZZcZuabmnXKtO1tuTpg8Iv3CCESC7pgSfAt+YsyuAlwABX8IbwtZt75efGPXh7uffc9KkfIkSqkQCeADnm6ii3nBZaX+S7jtmWzYm1ek2XtvFZjFUI0XQSwOOkorqW15ZsQWtNqVmfpF3QCjETj+nsuujYnKrlTb9xLI9eNlRWcxGiGZMx8DiY/NgnrNxmzGZs1yqTL7/dD0BedgZj+nW0UwZ/e/5gOuRmctvpR3POkO707th8Zhr269yGfubSY0KI5kl64DGmtbaDN8A/Ptlol4DNy8nkN+cZvez2uZl0a9fKnsXYnIK3ECI1SACPsT53vOO6vfCbvfZ2qyw/HcxqfiMLvbNNhBAiWjKEkiA/OdUo8JTfOotXfziagd0TvyK6EKJlkR54DNXVBVICzw9ax/LHpwRqcI/sk5/QBXWFEC2TBPAY+s5Tn9vbf7p8GAvvnGDfTkYOtxCiZZOoEkNfmxcvh/ZqD0DnvBzOHtKNk44qSGazhBAtlATwJvrDzDUM692e8QO62OtR/veGE+37//qd4clqmhCihZMA3gRrd5byF3MxhgxH+VefLLgrhEgAGQNvpM17y5n06Mf27Zo6qcEqhEgsCeCNNO7hOZ77L0nQAr9CCCFDKDF0+xlHc8PJ8VnQVwghgkkPvIk2PHCWvf3jU/pJ8SchRMJIAG+Emto6AIb1bo9fLlgKIZIk4hCKUqoX8DzQFagDpmmt/6yUygdeAQqBYuBSrfW++DW1+bCKU/UrMKr1fXL7qUlsjRAiXUXTA68Bfq61PgYYBfxEKTUQmArM1lr3B2abt9PCfTNWATDhGGO5sV75ufTKl2qCQojEihjAtdbbtdZfmNulwCqgB3Ae8Jx52HPA+fFqZHNzfGEHAM4Y3DXJLRFCpLMGjYErpQqBYcACoIvWejsYQR6I31LpzUxBXrYsdiCESLqoA7hSqg3wOnCz1vpgpOMdj7teKbVYKbW4pKSkMW1sdvaWVZFv1vUWQohkiSqAK6UyMYL3i1rrN8zdO5VS3cz7uwG7vB6rtZ6mtS7SWhcVFKRmUaddpRUUTp3B3+Ya0+b3lVXTPjczwqOEECK+IgZwZSQ2Pw2s0lr/0XHXm8AUc3sKMD32zWsevtp8AICH31sDwJqdpWRn+pPZJCGEiGom5hjgamC5Umqpue9XwEPAq0qp64BvgUvi08Tkq6iptbd3HqwAYHHx3nCHCyFEQkQM4FrrT4Fws1UmhNnfYtz88pf8b+k2+/YJD8wG4Mbx/cI9RAghEkJmYnrYuv8whVNn8MHXO13B22lgN1nTUgiRXBLAPazYaox5f//5xfa+wT3cAXtY7w4JbZMQQgSTAO7hhy8sCdl3zpDutGtlZJ6cPaRbopskhBAhpJxskMKpMzz3nz20O9eO7cPCb/YyXHrfQohmQAK4wyVPfua5/5PbT6VH+1YAjOnXKZFNEkKIsCSAOywqDhRTXHTnRDbvK2fV9oNSqEoI0SxJAHc4Z2h33lq2jW8ePAulFAV52TJcIoRotuQipsPOAxWMLMyXVXWEEClBArjDjoMVdGmXk+xmCCFEVCSAO+w+VElBm+xkN0MIIaIiAdx0qLKG8qpaCvIkgAshUoMEcJNVpKprOwngQojUIAHcZAXwLnkyBi6ESA0SwE2fb9gDIBcxhRApQwK4qbpOA9BNArgQIkVIADdV1dSRm+UnN0vmNgkhUoMEcNO+sio6yELFQogUIt1N0xtfbk12E4QQokGkBw5orZPdBCGEaDAJ4MDOg5UAXDe2T5JbIoQQ0ZMADmzdXw7A2P5S61sIkTokgAP7yqoB5CKmECKlSAAH9pVXAdAhNzPJLRFCiOhJAAf2lxs98PbSAxdCpBAJ4MCsr3cC0DZHsiqFEKlDAjiQ4TdW4JGVeIQQqUQCOFBTqzmhT36ymyGEEA0iARzYWVpBl7ZSxEoIkVrSPoBrrdlxoIIubWUhByFEakn7AH7wcA2VNXXSAxdCpJy0D+A7rJV4JIALIVKMBHAzgMtCDkKIVBMxgCulnlFK7VJKrXDsy1dKzVJKrTN/d4hvM+Pnq837AemBCyFSTzQ98H8CZwTtmwrM1lr3B2abt1PS619sASSACyFST8QArrX+GNgbtPs84Dlz+zng/Bi3K2E6tDamz2dlpP1okhAixTQ2anXRWm8HMH93DnegUup6pdRipdTikpKSRr5c/Hz57f5kN0EIIRol7t1OrfU0rXWR1rqooKAg3i/XIOt3lQJwWVGvJLdECCEarrEBfKdSqhuA+XtX7JqUOL/4z1cAjJRp9EKIFNTY8ntvAlOAh8zf02PWojibt343Vz61wLXvgmE9ktQaIYRovGjSCF8C5gNHK6W2KKWuwwjcpyml1gGnmbdTwuNz1ofs8/mkCqEQIvVE7IFrra8Ic9eEGLclIVpl+l23F/wqJU9DCCEaPYSSksqrapi9ehd5ORn8YtLRdG/fSvK/hRApK60C+MC73gdgUPe2TDmxMLmNEUKIJkrL2SuPXT4s2U0QQogmS5sAXlFdC0DXtjl0lmETIUQLkDYB/ItN+wB48MJjk9wSIYSIjbQJ4A+/vwYwxr+FEKIlSJsAvtQsG1uQJ0unCSFahrQI4Fc5Zl4qJZN2hBAtQ1oE8E/X7wbgtR+NTnJLhBAidlp8AC+rrAHgnKHdKSqUolVCiJajxQfwJ+ZuAKBPp9ZJbokQQsRWiw7gdXWav5rFq358ypFJbo0QQsRWiw7gv3z9K3s7J6iIlRBCpLoWHcD/s8RYsHjOL05JbkOEECIOWmwAr63T9raMfwshWqIWG8DfWb4dgM4ycUcI0UK12AB+00tfAjDtmqIkt0QIIeKjRQbw8Y/MtbeP69U+eQ0RQog4anEBvLKmlo27ywBYdtekJLdGCCHip8UF8Ase/wyASQO70C43M8mtEUKI+GkxS6pprbn66YV8vf0gAI9edlySWySEEPHVYnrgM5Zvt4tWDeiaR+vsFvPZJIQQnlI+ylVU13LqI3PZfqACgO+eWMg95w5KcquEECL+Uj6AD/j1e/b2OUO7S/AWQqSNlA7gG0oO2dvPfu94Tj26cxJbI4QQiZWyAbyqpo4Jf/gIgOk/GcNQyfcWQqSZlL2IefSv3wXg3KHdJXgLIdJSSvTA756+gufmb+J7Ywop7Niau99cCYBS8OfLJV1QCJGeUiKAb9l3GIBn5xW79s/5+SmySLEQIm2lxBDKP64p4szBXV37lt01iUIpEyuESGMp0QP3+RRPXDUi2c0QQohmJSV64EIIIUI1KYArpc5QSq1RSq1XSk2NVaOEEEJE1ugArpTyA48DZwIDgSuUUgNj1TAhhBD1a0oPfCSwXmu9UWtdBbwMnBebZgkhhIikKQG8B7DZcXuLuc9FKXW9UmqxUmpxSUlJE15OCCGEU1MCuFcCtg7ZofU0rXWR1rqooKCgCS8nhBDCqSkBfAvQy3G7J7Ctac0RQggRraYE8EVAf6VUH6VUFnA58GZsmiWEECISpXXIqEf0D1bqLOBPgB94Rmt9f4TjS4BNjXy5TsDuRj42laTDeabDOUJ6nGc6nCMk/zyP0FqHjEE3KYAnklJqsda6KNntiLd0OM90OEdIj/NMh3OE5nueMhNTCCFSlARwIYRIUakUwKcluwEJkg7nmQ7nCOlxnulwjtBMzzNlxsCFEEK4pVIPXAghhIMEcCGESFEpEcBTuWytUqqXUmqOUmqVUmqlUupn5v58pdQspdQ683cHc79SSj1mnutXSqnhjueaYh6/Tik1JVnnFI5Syq+U+lIp9bZ5u49SaoHZ3lfMCV8opbLN2+vN+wsdz3GHuX+NUur05JxJeEqp9kqp15RSq833dHRLey+VUreY/1ZXKKVeUkrltIT3Uin1jFJql1JqhWNfzN47pdQIpdRy8zGPKZWA9R611s36B2OS0AagL5AFLAMGJrtdDWh/N2C4uZ0HrMUov/swMNXcPxX4nbl9FvAuRq2ZUcACc38+sNH83cHc7pDs8ws611uBfwNvm7dfBS43t58EbjC3fww8aW5fDrxibg80399soI/5vvuTfV5B5/gc8H1zOwto35LeS4yCdN8ArRzv4XdbwnsJnAQMB1Y49sXsvQMWAqPNx7wLnBn3c0r2P5go/uijgfcdt+8A7kh2u5pwPtOB04A1QDdzXzdgjbn9d+AKx/FrzPuvAP7u2O86Ltk/GLVwZgPjgbfNf8S7gYzg9xF4HxhtbmeYx6ng99Z5XHP4AdqawU0F7W8x7yWBKqP55nvzNnB6S3kvgcKgAB6T9868b7Vjv+u4eP2kwhBKVGVrU4H59XIYsADoorXeDmD+7mweFu58m/vf4U/A7UCdebsjsF9rXWPedrbXPhfz/gPm8c39HPsCJcCz5lDRU0qp1rSg91JrvRV4BPgW2I7x3iyh5b2Xlli9dz3M7eD9cZUKATyqsrXNnVKqDfA6cLPW+mB9h3rs0/XsTzql1NnALq31Euduj0N1hPua7TmaMjC+gj+htR4GlGF87Q4n5c7THAM+D2PYozvQGmPVrWCp/l5G0tDzSsr5pkIAT/mytUqpTIzg/aLW+g1z906lVDfz/m7ALnN/uPNtzn+HMcC5SqlijJWZxmP0yNsrpTLMY5zttc/FvL8dsJfmfY5gtG+L1nqBefs1jIDekt7LicA3WusSrXU18AZwIi3vvbTE6r3bYm4H74+rVAjgKV221rwS/TSwSmv9R8ddbwLWFewpGGPj1v5rzKvgo4AD5le794FJSqkOZi9pkrkv6bTWd2ite2qtCzHenw+11lcCc4CLzcOCz9E694vN47W5/3Izs6EP0B/jwlCzoLXeAWxWSh1t7poAfE0Lei8xhk5GKaVyzX+71jm2qPfSISbvnXlfqVJqlPl3u8bxXPGT7IsKUV54OAsje2MDcGey29PAto/F+Cr1FbDU/DkLY5xwNrDO/J1vHq8wFoveACwHihzPdS2w3vz5XrLPLcz5nkIgC6Uvxn/a9cB/gGxzf455e715f1/H4+80z30NCbiK34jzOw5YbL6f/8PIRGhR7yVwL7AaWAG8gJFJkvLvJfASxrh+NUaP+bpYvndAkfk32wD8laCL3fH4kan0QgiRolJhCEUIIYQHCeBCCJGiJIALIUSKkgAuhBApSgK4EEKkKAngQgiRoiSACyFEivp/NzPUiR8FoBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "smoothed_rewards = []\n",
    "smooth_window = 50\n",
    "for i in range(smooth_window, len(all_rewards)-smooth_window):\n",
    "    smoothed_rewards.append(np.mean(all_rewards[i-smooth_window:i+smooth_window]))\n",
    "    \n",
    "plt.plot(range(len(smoothed_rewards)), smoothed_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/nstep_dddqn_breakout_r40.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
